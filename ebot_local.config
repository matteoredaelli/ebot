%% -*- mode: erlang -*-

[
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %% SASL config
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 {sasl, [
	 {sasl_error_logger, {file, "priv/log/sasl-error.log"}},
	 {errlog_type, error},
	 {error_logger_mf_dir, "priv/log/sasl"},      % Log directory
	 {error_logger_mf_maxbytes, 10485760},   % 10 MB max file size
	 {error_logger_mf_maxfiles, 5}           % 5 files max
	]
 },
 {ebot, [

	 %% !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
	 %% see EBOT options in ebot.app and add your changes here! 
	 %% !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

	 %% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	 %% CACHE
	 %% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	 %% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	 %% DATABASE
	 %% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	 %%
	 %% you need to set the db backend (COUCHDB or RIAK)
	 %% in src/ebot.hrl file
	 {db_hostname, "127.0.0.1"},
	 %% COUCHDB
	 {db_port, 5984},
	 %% RIAK 
	 %% {db_port, 8087}

	 %% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	 %% MQ
	 %% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	 %% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	 %% WEB
	 %% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	 %% -------------------------------------------------------------------------------------------------
	 %% normalize_url
	 %% -------------------------------------------------------------------------------------------------
	 %%
	 %% options: 
	 %%    add_final_slash
	 %%    to_lower_case : urls are case insensive and some web pages have links with some uppercase letters..
	 %%    without_internal_links
	 %%    without_queries,
	 %%    {max_depth, 2}
	 %%          the url path will be truncated to a max_depth path 
	 %%          http://www.redaelli.org/matteo/blog/a/ ->  http://www.redaelli.org/matteo/blog/
	 %%          should be the same as "tot_new_urls_queues" in ebot_mq.conf
	 %%	    you should also start at least one crawler for depth in [0,max_depth]. see "crawler_pools" in this file
	 %%    (TODO)  {remove_filename, false}
	 {normalize_url, [
			  {replace_string, [
					    %% http://www.gettyre.it/motoweb/XXX;jsessionid=250485C.sae_1
					    {";[A-Za-z0-9]+=[^&;?]+", ""},
					    %% some sites have newlines in url links: 
					    %% see http://opensource.linux-mirror.org/index.php
					    %% TODO maybe it still doesn t work
					    {"\n",""},
					    %% http://github.com/dizzyd/ibrowse
					    {"&quot\$",""}
					   ]},
			  add_final_slash,
			  {max_depth, 2}, 
			  to_lower_case,
			  without_internal_links,
			  without_queries
			 ]},
	 
	 %% -------------------------------------------------------------------------------------------------
	 %% tobe_saved_headers
	 %% -------------------------------------------------------------------------------------------------
	 %% headers (if exist) that will be saved in the database
	 {tobe_saved_headers, 
	  [
	   <<"content-length">>,
	   <<"content-type">>,
	   <<"server">>,
	   <<"x-powered-by">>
	  ]},
	 
	 %% -------------------------------------------------------------------------------------------------
	 %% mime_any_regexp
	 %% -------------------------------------------------------------------------------------------------
	 %%
	 %% the url will be analyzed if at least one regexp will be satisfied
	 %%
	 %% at least one regexp must be defined
	 
	 {mime_any_regexps, [
			     {match,   "^text/"}
			    ]
	 },

	 %% -------------------------------------------------------------------------------------------------
	 %% url_all_regexps
	 %% -------------------------------------------------------------------------------------------------
	 %%
	 %% the url will be analyzed if ALL regexps will be satisfied
	 %%
	 {url_all_regexps, [
			    {nomatch, "^https"},
			    {nomatch, "//.+//"},
			    {nomatch, "/bugs/"},
			    {nomatch, "viewcvs\\.cgi"},

			    %% Skipping Apache.org urls
			    {nomatch, "\\.apache\\..+/dist/"},
			    {nomatch, "/snapshots/"},
			    {nomatch, "^http://mail-archives"},
			    {nomatch, "bugs.+/.+"},
			    %% apache mirror sites.. TODO
			    {nomatch, "apache\\.fastbull\\.org/.+"},
			    
			    %% Skipping unwanted files
			    {nomatch, "\\.deb$"},
			    {nomatch, "\\.git$"},
			    {nomatch, "\\.tgz"},
			    {nomatch, "\\.jar$"},
			    {nomatch, "\\.rpm$"},
			    {nomatch, "\\.tar\\.gz"},
			    
			    % Skipping CVS repositories
			    {nomatch, "/cvs/\\."},

			    %% Skipping Github unseful pages
			    {nomatch, "github\\.+/issues"},
			    {nomatch, "gist\\.github\\.com"},
			    %% the page gives incomplete header
			    {nomatch, "svn\\.github\\.com"},
			    
			    %% Skipping Gitorious unseful pages
			    {nomatch, "git.+/merge_requests/"},
			    {nomatch, "git.+/commits/"},
			    {nomatch, "git.+/trees/"},
			    
			    %% Skipping Git repositories	  		
			    {nomatch, "git.+/commit/"},
			    {nomatch, "git.+/tree/"},

			    %% Skipping HG repositories
			    {nomatch, "/changeset/"},

			    %% Skipping SVN repositories
			    {nomatch, "svn.+/viewvc/.+/"},
			    {nomatch, "http://svn\\."},
			    {nomatch, "/branches"},
			    {nomatch, "/trunk"},
			    {nomatch, "/tags"}
			   ]},

	 %% -------------------------------------------------------------------------------------------------
	 %% url_any_regexp
	 %% -------------------------------------------------------------------------------------------------
	 %%
	 %% the url will be analyzed if at least one regexp will be satisfied
	 %% at least one regexp must be defined, for instance
	 %	{match,"."}
	 
	 {url_any_regexps, [
			    {match, "freshmeat\\.net"},
			    {match, "github\\.com"},
			    {match, "code\\.google\\.com"},
			    {match, "sourceforge\\.net"},
			    {match, "ohloh\\.net"},
			    {match, "bitbucket\\.org"},
			    % {nomatch, "\\.com"},
			    {match, "\\.org"},
			    {match, "\\.net"}
			   ]
	 },

	 %% -------------------------------------------------------------------------------------------------
	 %% obsolete_urls_after_day
	 %% -------------------------------------------------------------------------------------------------
	 %%
	 %% after how many days, an ur that is stored in the DB will become obsolete
	 
	 {obsolete_urls_after_days, 1},

	 %% -------------------------------------------------------------------------------------------------
	 %% save_referrals
	 %% -------------------------------------------------------------------------------------------------
	 %%
	 %% values: external, internal
	 
	 {save_referrals, [external]},
	 
	 %% -------------------------------------------------------------------------------------------------
	 %% send_body_to_mq
	 %% -------------------------------------------------------------------------------------------------
	 %%
	 %% if you need to run custom functions over the body of visited urls
	 %% you can set this parameter to true, and then create a AMQP consumer
	 %% that can read the {url, body} from the processed queue, analyze then and then (eventually) update 
	 %% the db with the new custom info 
	 
	 {send_body_to_mq, false},

	 %% -------------------------------------------------------------------------------------------------
	 %% crawlers_pool
	 %% -------------------------------------------------------------------------------------------------
	 %%
	 %% how many crawler threads will be started for each candidated url queue/depth
	 %% {crawler_pools, [{0,3},{1,2},{2,1}]} means
	 %% 3 crawlers will analyze urls got from AMQP queue ebot.new.0 that countains urls with depth==0 
	 %%   (ex. http://www.redaelli.org, http://www.redaelli.org/index.html)
	 %% 2 crawlers will analyze urls got from AMQP queue ebot.new.1 that countains urls with depth==1
	 %%   (ex. http://www.redaelli.org/matteo/, http://www.redaelli.org/matteo/index.html)
	 %% 1 crawlers will analyze urls got from AMQP queue ebot.new.2 that countains urls with depth==2

	 {crawler_pools, [{0,4}, {1,2}, {2,1}] },
	 
	 %% -------------------------------------------------------------------------------------------------
	 %% start_crawlers_at_boot
	 %% -------------------------------------------------------------------------------------------------
	 %%
	 %% are the crawlers started automatically at boot time? 
	 {start_crawlers_at_boot, false},

	 %% -------------------------------------------------------------------------------------------------
	 %% crawlers_sleep_time
	 %% -------------------------------------------------------------------------------------------------
	 %%
	 %% how many milliseconds will each crawler sleep between two url crawls?
	 %% this option is useful in order to avoid heavy workloads to teh visited sites
	 %% and for the ebot system if the hardware is not powerful enough

	 {crawlers_sleep_time, 10}
		 
	]
 }
].
