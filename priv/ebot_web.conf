%%
%%  WEB Specific options
%%
{http_header,	[
		{"User-Agent", "Mozilla/5.0 ebot/1"},
	      	{"Accept-Charset", "utf-8"},
	      	{"Accept", "text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain;q=0.8"},
	      	{"Accept-Language", "en-us,en;q=0.5"}
]}.

{http_options,	[
		{autoredirect, true}, 
	    	{timeout, 60000}
]}.


{request_options, [
%		  {proxy, {{"proxy.mycompany.com", 80}, ["localhost"]}}
		  {proxy, noproxy}
]}.


%%
%%  WEB CRAWLERS Specific options
%%

%% options: 
%%    without_internal_links
%%    without_queries,
%%    {max_depth, 2}
%%          should be the same as ebot_amqp.conf
%%	    the url path will be truncated to a max_depth path 
%%    (TODO)  {remove_filename, false}
{normalize_url, [
		{max_depth, 4}, 
		without_internal_links,
		without_queries
		]}.

%% at least one regexp must be defined

{mime_any_regexps, [
	{match,   "^text/"}
%	{nomatch, "^image/"},
%	{nomatch, "^application/"}
	]
}.

{url_all_regexps, [
	{nomatch,"gist\\.github\\.com"} 
	]}.

%% at least one regexp must be defined

{url_any_regexps, [
%	{match,"."}    
  	{match, "freshmeat\\.net"},
  	{match, "github\\.com"},
  	{match, "code\\.google\\.com"},
  	{match, "sourceforge\\.net"},
	{match, "ohloh\\.net"},
	{match, "bitbucket\\.org"}
%	{nomatch, "\\.com"},
%	{match, "\\.org"}
	]
}.


%% how many crawler threads will be started for each candidated url queue/depth
%% {crawler_pools, [{0,3},{1,2},{2,1}]}.

{crawler_pools, [{0,1}] }.	

